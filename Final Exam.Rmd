---
title: "Final Exam"
author: "Benjamin Lutz"
date: "12/4/2021"
output: html_document
---

## Problem 1

The first dataset (data1.txt) includes two variables, $\textit{Power_Generated}$ and $\textit{Wind_Speed}$. We want to investigate the relationship between the power generated (kwh) from wind mill turbine and the wind speed (m/s) at the time. Please fit a linear model to this data and analyze it thoroughly as you can, also answer the following questions. (50)

```{r}
powerData <- read.table("~/Desktop/linear_regression_5e_data_sets/data1.txt", header = T)

y = powerData$Power_Generated
x = powerData$Wind_Speed
```


**(a)** When you fit the model, determine which variable is the response and which is regressor.

```{r}
linMod1 <- lm(Power_Generated ~ Wind_Speed, data = powerData)

linMod1
```

Since the presence of wind determines whether power is generated, $\textit{Power_Generated}$ is the response and $\textit{Wind_Speed}$ is the regressor. Here is the fitted linear model.

\begin{equation*}
    \hat{y} = -13.600 + 4.663x
\end{equation*}

| 
| 

**(b)** When you fit the linear model and explain the intercept and slope, please do a hypothesis testing of $\beta_0 = 0$. Discuss this result. My guess is $\beta_0 < 0$, please try to explain why my assumption makes sense. Note that both of variables are positive.

| 
| 

The intercept is an approximation of how much power is generated (or lost) when there is no wind, and the slope is the rate at which power generated increases as wind speed increases. It is a good guess that $\beta_0 < 0$ because it is likely that power is lost through the idling system when there is no wind.

We first conduct hypothesis tests for $\beta_1 = 0$ and $\beta_0 = 0$.

```{r}
summary(linMod1)
```

For $\beta_0 = 0$ we have a p-value of $0+$, so $\beta_0 \neq 0$.

For $\beta_1 = 0$ we also have a p-value of $0+$, so $\beta_1 \neq 0$.

```{r}
confint(linMod1, level=0.95)  
```

A $95\%$ confidence interval on $\beta_1$ is 

\begin{equation*}
    (4.331721, 4.994699)
\end{equation*}

and a $95\%$ confidence interval on $\beta_0$ is 

\begin{equation*}
    (-16.074458, -11.125771)
\end{equation*}

The CI on $\beta_0$ indicates that your guess of $\beta_0 < 0$ is likely accurate.

| 
| 

**(c)** Remember to detect the model adequacy and outliers. Try to discuss the possible reasons of high influence points and outliers if you found some.

We have $\mbox{R}^2 = 0.5474$ and $\mbox{R}_{Adj}^2 = 0.5467$, so $54.7\%$ of the error is accounted for by the model. 

To test for outliers, we use the studentized residuals.

```{r}
library(car)

summary(rstandard(linMod1))

Boxplot(rstandard(linMod1))
```

The residuals for points $418, 419, 109, 107, 110, 105, 106, 104, 108, 103, 101, 99$ are outliers. These outliers may have been caused by occasional bad readings or by windmill troubleshooting or malfunctions.

We now test the model for influence points.

```{r}
summary(cooks.distance(linMod1))
```

Since the largest Cooks Distance is $2.502*10^{-2} < 1$, we conclude that there are no influential points.

We examine the following plots to test model adequacy.

```{r}
plot(linMod1)
```

The residual vs fitted plot indicates the residuals have non-constant variance, and the normal plot indicates a negative skew.

To potentially fix this, we can examine a plot of the response vs the regressor.

```{r}
plot(powerData$Wind_Speed, powerData$Power_Generated)
```

Based on this plot, it is possible that the data are exponentially related in which case a log transformation on the response would help the non constant variance of residuals.

```{r}
#Add 0.0000001 to y to address any y=0 scenarios
linMod1$ystar = log(y+0.0000001)
ystar = linMod1$ystar

linMod1B <- lm(ystar ~ x)
plot(linMod1B)
```

Since the residual vs response plot can be roughly contained between two horizontal lines, there has been an improvement on the non constant variance assumption.

| 
| 

**(d)** Could you extrapolate your model to a wider range of data? Explain why or why not.

The model has non constant variance and the normality assumption is violated. When model assumptions are violated, it is unsafe to extrapolate the model to a wider range of data. For our case in particular, the variance seems to grow substantially for larger fitted values, so additional terms outside the current range might have larger variance in the error terms.


| 
| 
| 

## Problem 2

I have a data set (data2.txt) to study the density of some material. I observe 3 different properties from lab (Prop1, Prop2, Prop3) and I guess these three properties may affect the density. Please fit a linear model and answer the following questions. (The same data set from our midterm)


**(a)** Fit a multiple linear model, you may need some analysis that have been done from midterm to continue.

```{r}
myData <- read.table("~/Desktop/linear_regression_5e_data_sets/data2.txt", header = TRUE)

Y = myData$density
X1 = myData$Prop1
X2 = myData$Prop2
X3 = myData$Prop3

linearModel <- lm(Y ~ X1 + X2 + X3)
linearModel
```

Fitting a multiple linear regression model, we get 

\begin{equation*}
    \hat{y} = 3.105192 + -1.054562x_1 + 0.103627x_2 + -0.001039x_3
\end{equation*}

```{r}
summary(linearModel)
```

Testing the hypotheses 

\begin{align*}
H_0:& \beta_1 = \beta_2 = \beta_3 = 0 \\

H_1:& \beta_j \neq 0 \mbox{ for at least one } j \in \{1, 2, 3\}
\end{align*}

yields the p-value, $2.2*10^{-16} < 0.05$.  Thus there is at least one regressor that contributes significantly to the model.

```{r}
confint(linearModel, level=0.95)
```

Confidence intervals on the regression coefficients are as follows.

$\beta_1$: (-1.229693496, -0.8794304378)

$\beta_2$: (0.101117084, 0.1061370667)

$\beta_3$: (-0.001601547, -0.0004757419)

We also see that 

\begin{align*}
    R^2 =& 0.9738 \\
    R^2_{Adj} =& 0.9736
\end{align*}

so $97.36\%$ of the variability is explained by this model.

| 
| 

**(b)** Check model adequacy based on what we have discussed. If you find some problems from your diagnostics, try to fix them. Provide your evidence if you conclude that the adequacy is satisfied.


```{r}
plot(linearModel)
```

We see that the residuals have a heavy-tailed distribution and there is non constant variance. We will examine plots of the response vs each regressor to determine the necessary transformations.

```{r}
pairs(myData)
```

We see that Prop1 and Prop3 may have an exponential relationship with the response. To account for this, we perform a log transformation on these regressors.

```{r}
transLinMod <- lm(density ~ log(Prop1) + Prop2 + log(Prop3), data = myData)

plot(transLinMod)
```

The variance of residuals has now been stabilized slightly, although the distribution of residuals is still heavy-tailed. Thus, model adequacy has been improved but is still unsatisfactory.

| 
| 

**(c)** What do you think about this data set?

```{r}
summary(cooks.distance(linearModel))
```

One note is that the largest value of Cooks Distance in this data set is $4.009*10^{-2} < 1$, so there are no influence points. Also, the following code shows there are several outliers.

```{r}
library(car)

summary(rstandard(linearModel))

Boxplot(rstandard(linearModel))
```

It makes sense that there would be outliers when examining the effect of certain properties on density because occasional applications of a property might be sloppy. 

It also seems to be a sufficiently large data set, so our observations about the model are likely to be useful.










